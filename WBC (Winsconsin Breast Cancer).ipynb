{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install tensorflow","execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting tensorflow\n  Downloading tensorflow-2.3.1-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\n\u001b[K     |████████████████████████████████| 320.4 MB 24 kB/s  eta 0:00:01��█████████    | 279.9 MB 2.6 MB/s eta 0:00:16��███████████████████▎   | 283.6 MB 2.6 MB/s eta 0:00:14��█▊ | 307.5 MB 2.6 MB/s eta 0:00:05\n\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n\u001b[K     |████████████████████████████████| 2.9 MB 58.9 MB/s eta 0:00:01\n\u001b[?25hCollecting tensorboard<3,>=2.3.0\n  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n\u001b[K     |████████████████████████████████| 6.8 MB 55.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: wheel>=0.26 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorflow) (0.34.2)\nCollecting wrapt>=1.11.1\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\nRequirement already satisfied: six>=1.12.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorflow) (1.15.0)\nCollecting gast==0.3.3\n  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\nCollecting numpy<1.19.0,>=1.16.0\n  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n\u001b[K     |████████████████████████████████| 20.1 MB 65.5 MB/s eta 0:00:01█████████████▉      | 16.2 MB 65.5 MB/s eta 0:00:01\n\u001b[?25hCollecting grpcio>=1.8.6\n  Downloading grpcio-1.33.2-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\n\u001b[K     |████████████████████████████████| 3.8 MB 24.9 MB/s eta 0:00:01\n\u001b[?25hCollecting absl-py>=0.7.0\n  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n\u001b[K     |████████████████████████████████| 127 kB 66.0 MB/s eta 0:00:01\n\u001b[?25hCollecting opt-einsum>=2.3.2\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001b[K     |████████████████████████████████| 65 kB 5.9 MB/s  eta 0:00:01\n\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n\u001b[K     |████████████████████████████████| 459 kB 62.3 MB/s eta 0:00:01\n\u001b[?25hCollecting protobuf>=3.9.2\n  Downloading protobuf-3.13.0-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n\u001b[K     |████████████████████████████████| 1.3 MB 58.9 MB/s eta 0:00:01\n\u001b[?25hCollecting google-pasta>=0.1.8\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n\u001b[K     |████████████████████████████████| 57 kB 6.8 MB/s  eta 0:00:01\n\u001b[?25hCollecting astunparse==1.6.3\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting keras-preprocessing<1.2,>=1.1.1\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[K     |████████████████████████████████| 42 kB 1.9 MB/s  eta 0:00:01\n\u001b[?25hCollecting termcolor>=1.1.0\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\nCollecting tensorboard-plugin-wit>=1.6.0\n  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n\u001b[K     |████████████████████████████████| 779 kB 54.0 MB/s eta 0:00:01\n\u001b[?25hCollecting werkzeug>=0.11.15\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n\u001b[K     |████████████████████████████████| 298 kB 61.4 MB/s eta 0:00:01\n\u001b[?25hCollecting markdown>=2.6.8\n  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n\u001b[K     |████████████████████████████████| 96 kB 5.9 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\nRequirement already satisfied: setuptools>=41.0.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0.post20200712)\nCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\nCollecting google-auth<2,>=1.6.3\n  Downloading google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n\u001b[K     |████████████████████████████████| 114 kB 66.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.10)\nRequirement already satisfied: idna<3,>=2.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\nRequirement already satisfied: chardet<4,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\nCollecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n\u001b[K     |████████████████████████████████| 47 kB 6.0 MB/s  eta 0:00:01\n\u001b[?25hCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n\u001b[K     |████████████████████████████████| 155 kB 48.6 MB/s eta 0:00:01\n\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\nRequirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.0.1)\nCollecting pyasn1>=0.1.3\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n\u001b[K     |████████████████████████████████| 77 kB 7.0 MB/s  eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: wrapt, termcolor\n  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69739 sha256=c4f9f4a5dbaf06e6c43f43698ac63e2d691da8c54662c5765870016804cc8693\n  Stored in directory: /home/jovyan/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=dd2b26619d488ce64c298acbcd2b9d51cf64dbba4d55950987b4e654666797bd\n  Stored in directory: /home/jovyan/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\nSuccessfully built wrapt termcolor\nInstalling collected packages: numpy, h5py, grpcio, tensorboard-plugin-wit, werkzeug, markdown, requests-oauthlib, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, absl-py, protobuf, tensorboard, wrapt, gast, opt-einsum, tensorflow-estimator, google-pasta, astunparse, keras-preprocessing, termcolor, tensorflow\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.19.1\n    Uninstalling numpy-1.19.1:\n      Successfully uninstalled numpy-1.19.1\nSuccessfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.33.2 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.18.5 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\nNote: you may need to restart the kernel to use updated packages.\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install keras","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting keras\n  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\nRequirement already satisfied: scipy>=0.14 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from keras) (1.5.2)\nRequirement already satisfied: pyyaml in /srv/conda/envs/notebook/lib/python3.6/site-packages (from keras) (5.3.1)\nRequirement already satisfied: numpy>=1.9.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from keras) (1.19.1)\nCollecting h5py\n  Downloading h5py-3.0.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[K     |████████████████████████████████| 4.0 MB 4.3 MB/s eta 0:00:01\n\u001b[?25hCollecting cached-property\n  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\nInstalling collected packages: cached-property, h5py, keras\nSuccessfully installed cached-property-1.5.2 h5py-3.0.0 keras-2.4.3\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nurl=\"https://bd29ee0e-54ab-4daa-9671-d153865d1620.usrfiles.com/ugd/bd29ee_7cd180d13fef452c9edb16585770fb1a.csv\"\ndf=pd.read_csv(url,header=None)\ndf.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"         0  1      2      3       4       5        6        7       8   \\\n0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n\n        9   ...     22     23      24      25      26      27      28      29  \\\n0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n\n       30       31  \n0  0.4601  0.11890  \n1  0.2750  0.08902  \n2  0.3613  0.08758  \n3  0.6638  0.17300  \n4  0.2364  0.07678  \n\n[5 rows x 32 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>...</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>...</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>...</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>...</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>...</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 32 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop(columns=0)\ndf.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"  1      2      3       4       5        6        7       8        9       10  \\\n0  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n1  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n2  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n3  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n4  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n\n   ...     22     23      24      25      26      27      28      29      30  \\\n0  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654  0.4601   \n1  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860  0.2750   \n2  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430  0.3613   \n3  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575  0.6638   \n4  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625  0.2364   \n\n        31  \n0  0.11890  \n1  0.08902  \n2  0.08758  \n3  0.17300  \n4  0.07678  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>...</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>...</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>...</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>...</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>...</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(569, 31)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(df.iloc[:,0]).count().T.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"1    B    M\n1  357  212\n2  357  212\n3  357  212\n4  357  212\n5  357  212","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>1</th>\n      <th>B</th>\n      <th>M</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>357</td>\n      <td>212</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>357</td>\n      <td>212</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>357</td>\n      <td>212</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>357</td>\n      <td>212</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>357</td>\n      <td>212</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop(columns=1)\nX.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"      2      3       4       5        6        7       8        9       10  \\\n0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n\n        11  ...     22     23      24      25      26      27      28      29  \\\n0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n\n       30       31  \n0  0.4601  0.11890  \n1  0.2750  0.08902  \n2  0.3613  0.08758  \n3  0.6638  0.17300  \n4  0.2364  0.07678  \n\n[5 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>...</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=pd.DataFrame(df.iloc[:,0]).rename(columns={1:\"Label\"})\ny.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"  Label\n0     M\n1     M\n2     M\n3     M\n4     M","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>M</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nX=pd.DataFrame(scaler.fit_transform(X))\nX.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"         0         1         2         3         4         5         6   \\\n0  1.097064 -2.073335  1.269934  0.984375  1.568466  3.283515  2.652874   \n1  1.829821 -0.353632  1.685955  1.908708 -0.826962 -0.487072 -0.023846   \n2  1.579888  0.456187  1.566503  1.558884  0.942210  1.052926  1.363478   \n3 -0.768909  0.253732 -0.592687 -0.764464  3.283553  3.402909  1.915897   \n4  1.750297 -1.151816  1.776573  1.826229  0.280372  0.539340  1.371011   \n\n         7         8         9   ...        20        21        22        23  \\\n0  2.532475  2.217515  2.255747  ...  1.886690 -1.359293  2.303601  2.001237   \n1  0.548144  0.001392 -0.868652  ...  1.805927 -0.369203  1.535126  1.890489   \n2  2.037231  0.939685 -0.398008  ...  1.511870 -0.023974  1.347475  1.456285   \n3  1.451707  2.867383  4.910919  ... -0.281464  0.133984 -0.249939 -0.550021   \n4  1.428493 -0.009560 -0.562450  ...  1.298575 -1.466770  1.338539  1.220724   \n\n         24        25        26        27        28        29  \n0  1.307686  2.616665  2.109526  2.296076  2.750622  1.937015  \n1 -0.375612 -0.430444 -0.146749  1.087084 -0.243890  0.281190  \n2  0.527407  1.082932  0.854974  1.955000  1.152255  0.201391  \n3  3.394275  3.893397  1.989588  2.175786  6.046041  4.935010  \n4  0.220556 -0.313395  0.613179  0.729259 -0.868353 -0.397100  \n\n[5 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.097064</td>\n      <td>-2.073335</td>\n      <td>1.269934</td>\n      <td>0.984375</td>\n      <td>1.568466</td>\n      <td>3.283515</td>\n      <td>2.652874</td>\n      <td>2.532475</td>\n      <td>2.217515</td>\n      <td>2.255747</td>\n      <td>...</td>\n      <td>1.886690</td>\n      <td>-1.359293</td>\n      <td>2.303601</td>\n      <td>2.001237</td>\n      <td>1.307686</td>\n      <td>2.616665</td>\n      <td>2.109526</td>\n      <td>2.296076</td>\n      <td>2.750622</td>\n      <td>1.937015</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.829821</td>\n      <td>-0.353632</td>\n      <td>1.685955</td>\n      <td>1.908708</td>\n      <td>-0.826962</td>\n      <td>-0.487072</td>\n      <td>-0.023846</td>\n      <td>0.548144</td>\n      <td>0.001392</td>\n      <td>-0.868652</td>\n      <td>...</td>\n      <td>1.805927</td>\n      <td>-0.369203</td>\n      <td>1.535126</td>\n      <td>1.890489</td>\n      <td>-0.375612</td>\n      <td>-0.430444</td>\n      <td>-0.146749</td>\n      <td>1.087084</td>\n      <td>-0.243890</td>\n      <td>0.281190</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.579888</td>\n      <td>0.456187</td>\n      <td>1.566503</td>\n      <td>1.558884</td>\n      <td>0.942210</td>\n      <td>1.052926</td>\n      <td>1.363478</td>\n      <td>2.037231</td>\n      <td>0.939685</td>\n      <td>-0.398008</td>\n      <td>...</td>\n      <td>1.511870</td>\n      <td>-0.023974</td>\n      <td>1.347475</td>\n      <td>1.456285</td>\n      <td>0.527407</td>\n      <td>1.082932</td>\n      <td>0.854974</td>\n      <td>1.955000</td>\n      <td>1.152255</td>\n      <td>0.201391</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.768909</td>\n      <td>0.253732</td>\n      <td>-0.592687</td>\n      <td>-0.764464</td>\n      <td>3.283553</td>\n      <td>3.402909</td>\n      <td>1.915897</td>\n      <td>1.451707</td>\n      <td>2.867383</td>\n      <td>4.910919</td>\n      <td>...</td>\n      <td>-0.281464</td>\n      <td>0.133984</td>\n      <td>-0.249939</td>\n      <td>-0.550021</td>\n      <td>3.394275</td>\n      <td>3.893397</td>\n      <td>1.989588</td>\n      <td>2.175786</td>\n      <td>6.046041</td>\n      <td>4.935010</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.750297</td>\n      <td>-1.151816</td>\n      <td>1.776573</td>\n      <td>1.826229</td>\n      <td>0.280372</td>\n      <td>0.539340</td>\n      <td>1.371011</td>\n      <td>1.428493</td>\n      <td>-0.009560</td>\n      <td>-0.562450</td>\n      <td>...</td>\n      <td>1.298575</td>\n      <td>-1.466770</td>\n      <td>1.338539</td>\n      <td>1.220724</td>\n      <td>0.220556</td>\n      <td>-0.313395</td>\n      <td>0.613179</td>\n      <td>0.729259</td>\n      <td>-0.868353</td>\n      <td>-0.397100</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle=preprocessing.LabelEncoder()\ny=pd.DataFrame(le.fit_transform(y))\ny.head()","execution_count":9,"outputs":[{"output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.6/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n","name":"stderr"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"   0\n0  1\n1  1\n2  1\n3  1\n4  1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import set_printoptions\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\ntest=SelectKBest(score_func=f_classif,k=\"all\")\nfit=test.fit(X,y)\nfeatures=fit.transform(X)\nset_printoptions(precision=3)\n\nscores=fit.scores_\nscores=pd.DataFrame(scores)\nscores=scores.rename(columns={0:\"scores\"})\nscores=scores.sort_values(by=[\"scores\"],ascending=False)\nscores","execution_count":10,"outputs":[{"output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.6/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n","name":"stderr"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"        scores\n27  964.385393\n22  897.944219\n7   861.676020\n20  860.781707\n2   697.235272\n23  661.600206\n0   646.981021\n3   573.060747\n6   533.793126\n26  436.691939\n5   313.233079\n25  304.341063\n10  268.840327\n12  253.897392\n13  243.651586\n21  149.596905\n24  122.472880\n28  118.860232\n1   118.096059\n17  113.262760\n4    83.651123\n8    69.527444\n29   66.443961\n15   53.247339\n16   39.014482\n19    3.468275\n14    2.557968\n9     0.093459\n11    0.039095\n18    0.024117","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>27</th>\n      <td>964.385393</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>897.944219</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>861.676020</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>860.781707</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>697.235272</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>661.600206</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>646.981021</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>573.060747</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>533.793126</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>436.691939</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>313.233079</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>304.341063</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>268.840327</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>253.897392</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>243.651586</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>149.596905</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>122.472880</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>118.860232</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>118.096059</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>113.262760</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>83.651123</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>69.527444</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>66.443961</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>53.247339</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>39.014482</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>3.468275</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2.557968</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.093459</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.039095</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.024117</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_scores=scores[scores[\"scores\"]>=35]\nX=X.iloc[:,x_scores.index]\nX.head()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"         27        22        7         20        2         23        0   \\\n0  2.296076  2.303601  2.532475  1.886690  1.269934  2.001237  1.097064   \n1  1.087084  1.535126  0.548144  1.805927  1.685955  1.890489  1.829821   \n2  1.955000  1.347475  2.037231  1.511870  1.566503  1.456285  1.579888   \n3  2.175786 -0.249939  1.451707 -0.281464 -0.592687 -0.550021 -0.768909   \n4  0.729259  1.338539  1.428493  1.298575  1.776573  1.220724  1.750297   \n\n         3         6         26  ...        21        24        28        1   \\\n0  0.984375  2.652874  2.109526  ... -1.359293  1.307686  2.750622 -2.073335   \n1  1.908708 -0.023846 -0.146749  ... -0.369203 -0.375612 -0.243890 -0.353632   \n2  1.558884  1.363478  0.854974  ... -0.023974  0.527407  1.152255  0.456187   \n3 -0.764464  1.915897  1.989588  ...  0.133984  3.394275  6.046041  0.253732   \n4  1.826229  1.371011  0.613179  ... -1.466770  0.220556 -0.868353 -1.151816   \n\n         17        4         8         29        15        16  \n0  0.660820  1.568466  2.217515  1.937015  1.316862  0.724026  \n1  0.260162 -0.826962  0.001392  0.281190 -0.692926 -0.440780  \n2  1.424827  0.942210  0.939685  0.201391  0.814974  0.213076  \n3  1.115007  3.283553  2.867383  4.935010  2.744280  0.819518  \n4  1.144205  0.280372 -0.009560 -0.397100 -0.048520  0.828471  \n\n[5 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>27</th>\n      <th>22</th>\n      <th>7</th>\n      <th>20</th>\n      <th>2</th>\n      <th>23</th>\n      <th>0</th>\n      <th>3</th>\n      <th>6</th>\n      <th>26</th>\n      <th>...</th>\n      <th>21</th>\n      <th>24</th>\n      <th>28</th>\n      <th>1</th>\n      <th>17</th>\n      <th>4</th>\n      <th>8</th>\n      <th>29</th>\n      <th>15</th>\n      <th>16</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.296076</td>\n      <td>2.303601</td>\n      <td>2.532475</td>\n      <td>1.886690</td>\n      <td>1.269934</td>\n      <td>2.001237</td>\n      <td>1.097064</td>\n      <td>0.984375</td>\n      <td>2.652874</td>\n      <td>2.109526</td>\n      <td>...</td>\n      <td>-1.359293</td>\n      <td>1.307686</td>\n      <td>2.750622</td>\n      <td>-2.073335</td>\n      <td>0.660820</td>\n      <td>1.568466</td>\n      <td>2.217515</td>\n      <td>1.937015</td>\n      <td>1.316862</td>\n      <td>0.724026</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.087084</td>\n      <td>1.535126</td>\n      <td>0.548144</td>\n      <td>1.805927</td>\n      <td>1.685955</td>\n      <td>1.890489</td>\n      <td>1.829821</td>\n      <td>1.908708</td>\n      <td>-0.023846</td>\n      <td>-0.146749</td>\n      <td>...</td>\n      <td>-0.369203</td>\n      <td>-0.375612</td>\n      <td>-0.243890</td>\n      <td>-0.353632</td>\n      <td>0.260162</td>\n      <td>-0.826962</td>\n      <td>0.001392</td>\n      <td>0.281190</td>\n      <td>-0.692926</td>\n      <td>-0.440780</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.955000</td>\n      <td>1.347475</td>\n      <td>2.037231</td>\n      <td>1.511870</td>\n      <td>1.566503</td>\n      <td>1.456285</td>\n      <td>1.579888</td>\n      <td>1.558884</td>\n      <td>1.363478</td>\n      <td>0.854974</td>\n      <td>...</td>\n      <td>-0.023974</td>\n      <td>0.527407</td>\n      <td>1.152255</td>\n      <td>0.456187</td>\n      <td>1.424827</td>\n      <td>0.942210</td>\n      <td>0.939685</td>\n      <td>0.201391</td>\n      <td>0.814974</td>\n      <td>0.213076</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.175786</td>\n      <td>-0.249939</td>\n      <td>1.451707</td>\n      <td>-0.281464</td>\n      <td>-0.592687</td>\n      <td>-0.550021</td>\n      <td>-0.768909</td>\n      <td>-0.764464</td>\n      <td>1.915897</td>\n      <td>1.989588</td>\n      <td>...</td>\n      <td>0.133984</td>\n      <td>3.394275</td>\n      <td>6.046041</td>\n      <td>0.253732</td>\n      <td>1.115007</td>\n      <td>3.283553</td>\n      <td>2.867383</td>\n      <td>4.935010</td>\n      <td>2.744280</td>\n      <td>0.819518</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.729259</td>\n      <td>1.338539</td>\n      <td>1.428493</td>\n      <td>1.298575</td>\n      <td>1.776573</td>\n      <td>1.220724</td>\n      <td>1.750297</td>\n      <td>1.826229</td>\n      <td>1.371011</td>\n      <td>0.613179</td>\n      <td>...</td>\n      <td>-1.466770</td>\n      <td>0.220556</td>\n      <td>-0.868353</td>\n      <td>-1.151816</td>\n      <td>1.144205</td>\n      <td>0.280372</td>\n      <td>-0.009560</td>\n      <td>-0.397100</td>\n      <td>-0.048520</td>\n      <td>0.828471</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(569, 25)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":13,"outputs":[{"output_type":"stream","text":"(398, 25)\n(171, 25)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nforest=RandomForestClassifier(n_estimators=25,criterion=\"gini\",max_depth=5,min_samples_split=3,min_samples_leaf=3,max_features=\"log2\",bootstrap=True,n_jobs=-1,warm_start=True,class_weight=\"balanced_subsample\",ccp_alpha=0.001,random_state=0).fit(X_train,y_train)\nforest","execution_count":14,"outputs":[{"output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  \n/srv/conda/envs/notebook/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n  warn('class_weight presets \"balanced\" or '\n","name":"stderr"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"RandomForestClassifier(ccp_alpha=0.001, class_weight='balanced_subsample',\n                       max_depth=5, max_features='log2', min_samples_leaf=3,\n                       min_samples_split=3, n_estimators=25, n_jobs=-1,\n                       random_state=0, warm_start=True)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ny_pred=forest.predict(X_test)\nprint(\"Accuracy on test data: \",accuracy_score(y_test,y_pred))","execution_count":15,"outputs":[{"output_type":"stream","text":"Accuracy on test data:  0.9824561403508771\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nprint(\"AUC on test data: \",roc_auc_score(y_test,y_pred))","execution_count":16,"outputs":[{"output_type":"stream","text":"AUC on test data:  0.9794973544973545\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\nimport matplotlib.pyplot as plt\nprint(\"Confusion matrix for test data: \\n\")\nplot_confusion_matrix(forest,X_test,y_test)\nplt.show()","execution_count":17,"outputs":[{"output_type":"stream","text":"Confusion matrix for test data: \n\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngradient=GradientBoostingClassifier(loss=\"exponential\",learning_rate=0.006,n_estimators=200,subsample=0.5,criterion=\"friedman_mse\",min_samples_split=3,min_samples_leaf=3,max_depth=3,max_features=\"sqrt\",ccp_alpha=0.0035,random_state=0).fit(X_train,y_train)\ngradient","execution_count":78,"outputs":[{"output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.6/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n","name":"stderr"},{"output_type":"execute_result","execution_count":78,"data":{"text/plain":"GradientBoostingClassifier(ccp_alpha=0.0035, learning_rate=0.006,\n                           loss='exponential', max_features='sqrt',\n                           min_samples_leaf=3, min_samples_split=3,\n                           n_estimators=200, random_state=0, subsample=0.5)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=gradient.predict(X_test)\nprint(\"Accuracy on test data: \",accuracy_score(y_test,y_pred))","execution_count":79,"outputs":[{"output_type":"stream","text":"Accuracy on test data:  0.9707602339181286\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nKNN=KNeighborsClassifier(n_neighbors=6,weights=\"distance\",algorithm=\"kd_tree\",leaf_size=50,p=1,n_jobs=-1).fit(X_train,y_train)\nKNN","execution_count":100,"outputs":[{"output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  \n","name":"stderr"},{"output_type":"execute_result","execution_count":100,"data":{"text/plain":"KNeighborsClassifier(algorithm='kd_tree', leaf_size=50, n_jobs=-1,\n                     n_neighbors=6, p=1, weights='distance')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=KNN.predict(X_test)\nprint(\"Accuracy on test data: \",accuracy_score(y_test,y_pred))","execution_count":101,"outputs":[{"output_type":"stream","text":"Accuracy on test data:  0.9707602339181286\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\nsvm=svm.SVC(C=0.45,kernel=\"linear\",degree=3,gamma=\"auto\",cache_size=90,\n            decision_function_shape=\"ovo\",probability=True,random_state=0).fit(X_train,y_train)\nsvm","execution_count":129,"outputs":[{"output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.6/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n","name":"stderr"},{"output_type":"execute_result","execution_count":129,"data":{"text/plain":"SVC(C=0.45, cache_size=90, decision_function_shape='ovo', gamma='auto',\n    kernel='linear', probability=True, random_state=0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=svm.predict(X_test)\nprint(\"Accuracy on test data: \",accuracy_score(y_test,y_pred))","execution_count":130,"outputs":[{"output_type":"stream","text":"Accuracy on test data:  0.9883040935672515\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AUC on test data: \",roc_auc_score(y_test,y_pred))","execution_count":144,"outputs":[{"output_type":"stream","text":"AUC on test data:  0.9841269841269842\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(svm,X_test,y_test)\nplt.show()","execution_count":146,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=svm.predict_proba(X_test)\ny_pred=pd.DataFrame(y_prob).rename(columns={0:\"pred_0\",1:\"pred_1\"})\ny_pred.head()","execution_count":148,"outputs":[{"output_type":"execute_result","execution_count":148,"data":{"text/plain":"         pred_0    pred_1\n0  8.735124e-01  0.126488\n1  3.645965e-08  1.000000\n2  1.036889e-02  0.989631\n3  9.977960e-01  0.002204\n4  9.997428e-01  0.000257","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pred_0</th>\n      <th>pred_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.735124e-01</td>\n      <td>0.126488</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.645965e-08</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.036889e-02</td>\n      <td>0.989631</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9.977960e-01</td>\n      <td>0.002204</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9.997428e-01</td>\n      <td>0.000257</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_probs1=y_pred[[\"pred_1\"]]","execution_count":149,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nthresholds=np.arange(0,1,0.001)","execution_count":150,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_labels(pos_probs,threshold):\n    return (pos_probs >= threshold).astype('int')","execution_count":151,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nscores=[f1_score(y_test, to_labels(y_probs1,t)) for t in thresholds]","execution_count":152,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ix=np.argmax(scores)\nprint((thresholds[ix],scores[ix]))","execution_count":153,"outputs":[{"output_type":"stream","text":"(0.461, 0.9919999999999999)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_probs1[y_probs1[\"pred_1\"]>=0.461])","execution_count":154,"outputs":[{"output_type":"execute_result","execution_count":154,"data":{"text/plain":"62"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[\"pred_0\"]=np.where((y_pred.pred_0 >=0.461),1,y_pred.pred_0)\ny_pred[\"pred_0\"]=np.where((y_pred.pred_0 < 0.461),0,y_pred.pred_0)\ny_pred[\"pred_1\"]=np.where((y_pred.pred_1 >=0.461),1,y_pred.pred_1)\ny_pred[\"pred_1\"]=np.where((y_pred.pred_1 <0.461),0,y_pred.pred_1)","execution_count":155,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[\"pred_0\"]=np.where((y_pred.pred_0 == y_pred.pred_1),0,y_pred.pred_0)","execution_count":159,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len11=len(y_pred[y_pred[\"pred_1\"]==1])\nprint(\"The number of predicted positive cases: \",len11)","execution_count":156,"outputs":[{"output_type":"stream","text":"The number of predicted positive cases:  62\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len00=len(y_pred[y_pred[\"pred_0\"]==1])\nprint(\"The number of predicted negative cases: \",len00)","execution_count":160,"outputs":[{"output_type":"stream","text":"The number of predicted negative cases:  109\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nienc=OneHotEncoder(handle_unknown=\"ignore\")\ny_test=ienc.fit_transform(y_test)\ny_test=pd.DataFrame(y_test.toarray())","execution_count":161,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test=ienc.inverse_transform(y_test)\ny_pred=ienc.inverse_transform(y_pred)","execution_count":162,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy on test data after adjusting the threshold: \",accuracy_score(y_test,y_pred))","execution_count":163,"outputs":[{"output_type":"stream","text":"Accuracy on test data after adjusting the threshold:  0.9941520467836257\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AUC on test data after adjusting the threshold: \",roc_auc_score(y_test,y_pred))","execution_count":164,"outputs":[{"output_type":"stream","text":"AUC on test data after adjusting the threshold:  0.9920634920634921\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(\"Confusion matrix for test data after adjusting the threshold: \\n\",confusion_matrix(y_test,y_pred))","execution_count":165,"outputs":[{"output_type":"stream","text":"Confusion matrix for test data after adjusting the threshold: \n [[108   0]\n [  1  62]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nienc=OneHotEncoder(handle_unknown=\"ignore\")\ny=ienc.fit_transform(y)\ny=pd.DataFrame(y.toarray())\ny.head()","execution_count":155,"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"     0    1\n0  0.0  1.0\n1  0.0  1.0\n2  0.0  1.0\n3  0.0  1.0\n4  0.0  1.0"},"execution_count":155,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":156,"outputs":[{"name":"stdout","output_type":"stream","text":"(398, 25)\n(171, 25)\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"length=X_train.shape[1]\nnum_classes=y_test.shape[1]","execution_count":157,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras import metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":158,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classification_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(length, activation='relu', input_shape=(length,)))\n    model.add(Dense(60, activation='relu'))\n    model.add(Dropout(0.4))\n    model.add(Dense(60,activation=\"relu\"))\n    model.add(Dropout(0.4))\n    model.add(Dense(60,activation=\"relu\"))\n    model.add(Dropout(0.4))\n    model.add(Dense(60,activation=\"relu\"))\n    model.add(Dropout(0.4))\n    model.add(Dense(60,activation=\"tanh\"))\n    model.add(Dense(num_classes, activation='softmax'))\n      \n    # compile model\n    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","execution_count":167,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=classification_model()\nmodel.fit(X_train,y_train,validation_data=(X_test, y_test),epochs=30, verbose=2)\nmodel.evaluate(X_test,y_test,verbose=0)","execution_count":169,"outputs":[{"name":"stdout","output_type":"stream","text":"Epoch 1/30\n13/13 - 0s - loss: 0.6944 - accuracy: 0.5879 - val_loss: 0.5917 - val_accuracy: 0.8246\nEpoch 2/30\n13/13 - 0s - loss: 0.5985 - accuracy: 0.7337 - val_loss: 0.4422 - val_accuracy: 0.9123\nEpoch 3/30\n13/13 - 0s - loss: 0.4553 - accuracy: 0.8291 - val_loss: 0.2407 - val_accuracy: 0.9415\nEpoch 4/30\n13/13 - 0s - loss: 0.3126 - accuracy: 0.8995 - val_loss: 0.1107 - val_accuracy: 0.9591\nEpoch 5/30\n13/13 - 0s - loss: 0.2162 - accuracy: 0.9171 - val_loss: 0.0722 - val_accuracy: 0.9649\nEpoch 6/30\n13/13 - 0s - loss: 0.2128 - accuracy: 0.9246 - val_loss: 0.0609 - val_accuracy: 0.9649\nEpoch 7/30\n13/13 - 0s - loss: 0.1456 - accuracy: 0.9397 - val_loss: 0.0511 - val_accuracy: 0.9766\nEpoch 8/30\n13/13 - 0s - loss: 0.1411 - accuracy: 0.9523 - val_loss: 0.0518 - val_accuracy: 0.9766\nEpoch 9/30\n13/13 - 0s - loss: 0.1248 - accuracy: 0.9623 - val_loss: 0.0528 - val_accuracy: 0.9766\nEpoch 10/30\n13/13 - 0s - loss: 0.1016 - accuracy: 0.9598 - val_loss: 0.0470 - val_accuracy: 0.9825\nEpoch 11/30\n13/13 - 0s - loss: 0.1117 - accuracy: 0.9673 - val_loss: 0.0439 - val_accuracy: 0.9825\nEpoch 12/30\n13/13 - 0s - loss: 0.0932 - accuracy: 0.9648 - val_loss: 0.0419 - val_accuracy: 0.9883\nEpoch 13/30\n13/13 - 0s - loss: 0.1024 - accuracy: 0.9698 - val_loss: 0.0426 - val_accuracy: 0.9825\nEpoch 14/30\n13/13 - 0s - loss: 0.1075 - accuracy: 0.9648 - val_loss: 0.0453 - val_accuracy: 0.9825\nEpoch 15/30\n13/13 - 0s - loss: 0.1098 - accuracy: 0.9548 - val_loss: 0.0483 - val_accuracy: 0.9883\nEpoch 16/30\n13/13 - 0s - loss: 0.0969 - accuracy: 0.9698 - val_loss: 0.0560 - val_accuracy: 0.9766\nEpoch 17/30\n13/13 - 0s - loss: 0.0987 - accuracy: 0.9673 - val_loss: 0.0473 - val_accuracy: 0.9825\nEpoch 18/30\n13/13 - 0s - loss: 0.0778 - accuracy: 0.9824 - val_loss: 0.0422 - val_accuracy: 0.9883\nEpoch 19/30\n13/13 - 0s - loss: 0.0740 - accuracy: 0.9774 - val_loss: 0.0403 - val_accuracy: 0.9883\nEpoch 20/30\n13/13 - 0s - loss: 0.0665 - accuracy: 0.9849 - val_loss: 0.0446 - val_accuracy: 0.9883\nEpoch 21/30\n13/13 - 0s - loss: 0.0607 - accuracy: 0.9799 - val_loss: 0.0405 - val_accuracy: 0.9883\nEpoch 22/30\n13/13 - 0s - loss: 0.0487 - accuracy: 0.9874 - val_loss: 0.0391 - val_accuracy: 0.9942\nEpoch 23/30\n13/13 - 0s - loss: 0.0717 - accuracy: 0.9774 - val_loss: 0.0432 - val_accuracy: 0.9883\nEpoch 24/30\n13/13 - 0s - loss: 0.0488 - accuracy: 0.9824 - val_loss: 0.0588 - val_accuracy: 0.9766\nEpoch 25/30\n13/13 - 0s - loss: 0.0553 - accuracy: 0.9874 - val_loss: 0.0559 - val_accuracy: 0.9825\nEpoch 26/30\n13/13 - 0s - loss: 0.0538 - accuracy: 0.9824 - val_loss: 0.0618 - val_accuracy: 0.9766\nEpoch 27/30\n13/13 - 0s - loss: 0.0399 - accuracy: 0.9824 - val_loss: 0.0668 - val_accuracy: 0.9766\nEpoch 28/30\n13/13 - 0s - loss: 0.0375 - accuracy: 0.9849 - val_loss: 0.0564 - val_accuracy: 0.9766\nEpoch 29/30\n13/13 - 0s - loss: 0.0328 - accuracy: 0.9899 - val_loss: 0.0511 - val_accuracy: 0.9825\nEpoch 30/30\n13/13 - 0s - loss: 0.0291 - accuracy: 0.9874 - val_loss: 0.0470 - val_accuracy: 0.9942\n"},{"data":{"text/plain":"[0.04696205258369446, 0.9941520690917969]"},"execution_count":169,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict(X_test)\nprint(\"AUC on test data: \",roc_auc_score(y_test,y_pred))","execution_count":170,"outputs":[{"name":"stdout","output_type":"stream","text":"AUC on test data:  0.9975014697236919\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=ienc.inverse_transform(y_pred)\ny_test=ienc.inverse_transform(y_test)","execution_count":172,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(\"Confusion matrix for test data: \\n\",confusion_matrix(y_test,y_pred))","execution_count":174,"outputs":[{"name":"stdout","output_type":"stream","text":"Confusion matrix for test data: \n [[108   0]\n [  1  62]]\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}